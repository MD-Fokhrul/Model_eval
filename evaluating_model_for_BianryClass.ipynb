{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model evaluating screach and sklearn\n",
    "> **Quetion:** Why model evaluation?\n",
    "\n",
    "**Answer:** To know how much our model doing well. And there are many indicator to determine model evalution. We will cover four indicaticator. They are Accuracy, Precission, Recall, F1-score (for more visit <a href=\"https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\"> here </a> ). \n",
    "\n",
    "Using <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a> modeule, we can do our task easily and efficiently. We will also disscuss <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a> also.\n",
    "\n",
    "By end of the lesson you can get foundamental idea to implement those term by screach and scikit-learn.\n",
    "\n",
    "\n",
    "### Let's try to implemnt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90422739 0.08304298 0.61560693 0.21994484 0.94452777 0.57191489\n",
      " 0.98667614 0.82460015 0.54792446 0.6471543  0.98654083 0.19120254\n",
      " 0.10194262 0.39718247 0.68868712 0.95801129 0.37098989 0.67438211\n",
      " 0.9925231  0.41204275 0.18471426 0.13218306 0.06045579 0.58677971\n",
      " 0.96958893 0.49135323 0.26957744 0.6112823  0.92200486 0.18123964\n",
      " 0.21937901 0.33001641 0.30204498 0.40291058 0.223754   0.6601223\n",
      " 0.44500938 0.73951507 0.55036338 0.52449551 0.93860003 0.23924698\n",
      " 0.86272215 0.51910164 0.79476111 0.07094065 0.23237571 0.32637492\n",
      " 0.18650239 0.48454701 0.53580101 0.46010946 0.52222656 0.27010628\n",
      " 0.44151522 0.64514849 0.64443742 0.96299991 0.22432383 0.02204425\n",
      " 0.39794616 0.09432731 0.30394979 0.76443263 0.931415   0.04859238\n",
      " 0.01789789 0.40249125 0.93380055 0.87255801 0.2618449  0.37635417\n",
      " 0.61149331 0.64301951 0.38548724 0.66089245 0.55350764 0.90033663\n",
      " 0.06198851 0.9430222  0.10386908 0.7708182  0.43103622 0.97211013\n",
      " 0.6237951  0.62266833 0.51480768 0.68453636 0.35615776 0.33797362\n",
      " 0.74997131 0.01154962 0.52161147 0.90971944 0.92241362 0.00107399\n",
      " 0.59174145 0.20502627 0.96199854 0.22242373]\n"
     ]
    }
   ],
   "source": [
    "# rendomly generate some number\n",
    "\n",
    "y_pred = np.random.random((100,))\n",
    "#help() \n",
    "print(y_pred)\n",
    "y = np.random.random((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9042273925594546\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>=0.5:\n",
    "        y_pred[i] = int(1)\n",
    "    else:\n",
    "        y_pred[i] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i]>=0.5:\n",
    "        y[i] = int(1)\n",
    "    else:\n",
    "        y[i] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0.]\n",
      "[0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# assume this two is our real and predicited label\n",
    "print(y_pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next cell we will implement our confusion matrix function which is key element for evaluating a model \n",
    "\n",
    "i.e. this may look different than confusion matrix we build later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusion_matrix(y, y_pred):\n",
    "    matrix = np.zeros((2,2))\n",
    "    for i in range(len(y)):\n",
    "        if y[i]==1 and y_pred[i]==1:\n",
    "            matrix[0][0] +=1\n",
    "        elif y[i]==0 and y_pred[i]==1:\n",
    "            matrix[0][1] +=1\n",
    "        elif y[i]==1 and y_pred[i]==0:\n",
    "            matrix[1][0] +=1\n",
    "        elif y[i]==0 and y_pred[i]==0:\n",
    "            matrix[1][1] +=1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32., 20.],\n",
       "       [23., 25.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_model(cm):\n",
    "    accuracy = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\n",
    "    precision = (cm[0][0])/(cm[0][0] + cm[0][1])\n",
    "    recall = (cm[0][0])/(cm[0][0] + cm[1][0])\n",
    "    f1_score = (2*precision*recall) / (precision + recall)\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff = evaluating_model(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.57,\n",
       " 'precision': 0.6153846153846154,\n",
       " 'recall': 0.5818181818181818,\n",
       " 'f1_score': 0.5981308411214953}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss # where y_pred are probabilities and y_true are binary class labels\n",
    "loss = log_loss(y, y_pred, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.851833769297633"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 20],\n",
       "       [23, 32]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1_score = f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.57 \n",
      " precision : 0.6153846153846154\n",
      " recall : 0.5818181818181818\n",
      " f1_score : 0.5981308411214953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy : {} \\n precision : {}\\n recall : {}\\n f1_score : {}\\n\".format(accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you look  our scrsach and sklearn result is same. that's mearn we successfully implement our model eval parameter.\n",
    "### Thanks all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
